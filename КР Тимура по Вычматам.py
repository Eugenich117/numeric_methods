"""
Контрольная работа №2, Вариант 8
Решение трёх задач по численным методам с подробными комментариями

Задачи:
1. Матричные вычисления: корень степени p из матрицы 2×2
2. Регрессия и регуляризация: линейная модель с L1/L2-регуляризацией
3. Вавилонский метод и метод Ньютона для вычисления квадратного корня

Автор: [Ваше имя]
Дата: [Текущая дата]
"""

# ============================================================================
# ИМПОРТ БИБЛИОТЕК
# ============================================================================

import numpy as np                                    # Матричные операции, массивы
import matplotlib.pyplot as plt                       # Построение графиков
import time                                           # Измерение времени выполнения
from scipy.linalg import fractional_matrix_power, norm # Эталонное решение для матричного корня и норма
from sklearn.linear_model import LinearRegression, Ridge, Lasso  # Регрессионные модели
from sklearn.model_selection import KFold             # Кросс-валидация
from sklearn.metrics import mean_squared_error        # Метрика качества модели
from sklearn.preprocessing import StandardScaler      # Нормализация признаков

# Фиксируем seed для воспроизводимости результатов
np.random.seed(42)

print("=" * 80)
print("КОНТРОЛЬНАЯ РАБОТА №2, ВАРИАНТ 8")
print("=" * 80)

# ============================================================================
# ЗАДАЧА 1: МАТРИЧНЫЕ ВЫЧИСЛЕНИЯ - КОРЕНЬ СТЕПЕНИ P ИЗ МАТРИЦЫ 2×2
# ============================================================================

print("\n" + "=" * 80)
print("ЗАДАЧА 1: МАТРИЧНЫЕ ВЫЧИСЛЕНИЯ")
print("=" * 80)

# ----------------------------------------------------------------------------
# Пункт 1: Функция вычисления корня степени p из матрицы 2×2
# ----------------------------------------------------------------------------

def matrix_pth_root(A, p):
    """
    ВЫЧИСЛЕНИЕ КОРНЯ СТЕПЕНИ P ИЗ МАТРИЦЫ 2×2 ПО ЯВНОЙ ФОРМУЛЕ

    ОСНОВНАЯ ИДЕЯ:
    Для матрицы 2×2 можно вычислить функцию f(A) через её собственные значения.
    Используется формула Сильвестра-Лагранжа для интерполяции полиномов.

    ТЕОРЕТИЧЕСКОЕ ОБОСНОВАНИЕ:
    Любая аналитическая функция от матрицы 2×2 может быть выражена как:
    f(A) = c0·I + c1·A, где коэффициенты c0, c1 находятся из системы:
    f(λ₁) = c0 + c1·λ₁
    f(λ₂) = c0 + c1·λ₂
    Решая эту систему, получаем явные формулы.

    Параметры:
    ----------
    A : ndarray (2, 2)
        Матрица, из которой извлекается корень
    p : int или float
        Степень корня (p > 0)

    Возвращает:
    -----------
    ndarray (2, 2)
        Матрица A^(1/p)

    Исключения:
    -----------
    ValueError: если p ≤ 0, либо если матрица имеет отрицательные собственные
                значения при чётном p (требуется комплексная арифметика)
    """
    print(f"Вычисление корня степени {p} из матрицы:")
    print(f"A = {A}")

    # ПРОВЕРКА КОРРЕКТНОСТИ ПАРАМЕТРОВ
    if p <= 0:
        raise ValueError("Степень p должна быть положительной")

    # ОСОБЫЙ СЛУЧАЙ: КОРЕНЬ СТЕПЕНИ 1
    if p == 1:
        print("Корень степени 1 - возвращаем исходную матрицу")
        return A

    # ОСОБЫЙ СЛУЧАЙ: НУЛЕВАЯ МАТРИЦА
    if np.allclose(A, 0):
        print("Нулевая матрица - возвращаем нулевую матрицу")
        return np.zeros((2, 2))

    # ВЫЧИСЛЯЕМ ОСНОВНЫЕ ХАРАКТЕРИСТИКИ МАТРИЦЫ
    # След матрицы: tr(A) = a₁₁ + a₂₂
    tr = np.trace(A)
    # Определитель матрицы: det(A) = a₁₁·a₂₂ - a₁₂·a₂₁
    det = np.linalg.det(A)

    print(f"След матрицы: tr(A) = {tr}")
    print(f"Определитель матрицы: |A| = {det}")

    # Среднее арифметическое собственных значений
    # По теореме Виета: λ₁ + λ₂ = tr(A), поэтому (λ₁ + λ₂)/2 = tr(A)/2
    mean_tr = tr / 2

    # Дискриминант характеристического уравнения
    # Характеристическое уравнение: λ² - tr(A)·λ + det(A) = 0
    # Дискриминант: D = tr(A)² - 4·det(A)
    # Полудискриминант: d = (tr(A)/2)² - det(A) = (λ₁ - λ₂)²/4
    d = mean_tr**2 - det

    print(f"Дискриминант: d = {d}")

    # --------------------------------------------------------------------
    # СЛУЧАЙ 1: КРАТНЫЕ СОБСТВЕННЫЕ ЗНАЧЕНИЯ (d ≈ 0, λ₁ = λ₂ = λ)
    # --------------------------------------------------------------------
    if abs(d) < 1e-12:
        print("Случай кратных собственных значений")
        # λ = tr(A)/2 (кратное собственное значение)
        lambda_val = mean_tr

        print(f"Кратное собственное значение: λ = {lambda_val}")

        # ПРОВЕРКА НА КОМПЛЕКСНЫЙ РЕЗУЛЬТАТ
        # Для корня чётной степени из отрицательного числа нужны комплексные числа
        if p % 2 == 0 and lambda_val < 0:
            raise ValueError(f"Для кратного отрицательного собственного значения λ={lambda_val} "
                           f"и чётного p={p} требуется комплексный корень")

        # ВЫЧИСЛЯЕМ ФУНКЦИЮ И ЕЁ ПРОИЗВОДНУЮ В ТОЧКЕ λ
        # f(x) = x^(1/p) - корень степени p
        if lambda_val >= 0:
            f_val = lambda_val**(1/p)  # f(λ)
        else:
            # Для отрицательного λ при нечётном p: f(λ) = -|λ|^(1/p)
            f_val = -((-lambda_val)**(1/p))

        # f'(x) = (1/p)·x^(1/p - 1) - производная
        if lambda_val != 0:
            f_derivative = (1/p) * lambda_val**(1/p - 1)
        else:
            f_derivative = 0  # Особый случай для λ = 0

        print(f"f(λ) = {f_val}, f'(λ) = {f_derivative}")

        # ФОРМУЛА ДЛЯ КРАТНЫХ СОБСТВЕННЫХ ЗНАЧЕНИЙ:
        # f(A) = f(λ)·I + f'(λ)·(A - λ·I)
        result = f_val * np.eye(2) + f_derivative * (A - lambda_val * np.eye(2))

        print("Результат по формуле для кратных собственных значений:")
        print(result)
        return result

    # --------------------------------------------------------------------
    # СЛУЧАЙ 2: РАЗЛИЧНЫЕ СОБСТВЕННЫЕ ЗНАЧЕНИЯ (d > 0, λ₁ ≠ λ₂)
    # --------------------------------------------------------------------
    else:
        print("Случай различных собственных значений")
        # Корень из дискриминанта (полуразность собственных значений)
        sqrt_d = np.sqrt(d)

        # СОБСТВЕННЫЕ ЗНАЧЕНИЯ МАТРИЦЫ A:
        # λ₁ = mean_tr + sqrt_d
        # λ₂ = mean_tr - sqrt_d
        l1 = mean_tr + sqrt_d
        l2 = mean_tr - sqrt_d

        print(f"Собственные значения: λ₁ = {l1}, λ₂ = {l2}")

        # ПРОВЕРКА НА КОМПЛЕКСНЫЙ РЕЗУЛЬТАТ
        # Для корня чётной степени из отрицательного числа нужны комплексные числа
        if p % 2 == 0 and (l1 < 0 or l2 < 0):
            raise ValueError(f"Для отрицательных собственных значений и чётного p={p} "
                           f"требуется комплексный корень")

        # ВЫЧИСЛЯЕМ f(λ) ДЛЯ КАЖДОГО СОБСТВЕННОГО ЗНАЧЕНИЯ
        def compute_f(l):
            """Вычисляет f(λ) = λ^(1/p) с учётом знака"""
            if l >= 0:
                return l**(1/p)
            else:
                # Для отрицательного λ при нечётном p: f(λ) = -|λ|^(1/p)
                return -((-l)**(1/p))

        f1 = compute_f(l1)  # f(λ₁)
        f2 = compute_f(l2)  # f(λ₂)

        print(f"f(λ₁) = {f1}, f(λ₂) = {f2}")

        # ФОРМУЛА СИЛЬВЕСТРА ДЛЯ ФУНКЦИИ ОТ МАТРИЦЫ:
        # f(A) = [(f(λ₁) + f(λ₂))/2]·I + [(A - mean_tr·I) / sqrt_d]·[(f(λ₁) - f(λ₂))/2]
        #
        # ГЕОМЕТРИЧЕСКАЯ ИНТЕРПРЕТАЦИЯ:
        # Первое слагаемое: среднее значение функции на собственных значениях
        # Второе слагаемое: направленная разность, масштабированная через (A - mean_tr·I)

        term1 = ((f1 + f2) / 2) * np.eye(2)                    # Постоянная составляющая
        term2 = (A - mean_tr * np.eye(2)) / sqrt_d             # Матричный коэффициент
        term3 = ((f1 - f2) / 2)                                # Скалярный коэффициент

        result = term1 + term2 * term3

        print("Результат по формуле Сильвестра:")
        print(result)
        return result

# ----------------------------------------------------------------------------
# Пункт 2: График зависимости времени работы от количества матриц
# ----------------------------------------------------------------------------

def time_experiment(p=3, max_n=10000):
    """
    ИССЛЕДОВАНИЕ МАСШТАБИРУЕМОСТИ АЛГОРИТМА

    ЦЕЛЬ: Оценить, как время вычисления зависит от количества обрабатываемых матриц.
    Это важно для понимания вычислительной сложности алгоритма.

    МЕТОДИКА:
    - Генерируем различное количество случайных матриц
    - Для каждой матрицы вычисляем корень степени p
    - Измеряем общее время выполнения
    - Строим график зависимости времени от количества матриц

    ОЖИДАЕМЫЙ РЕЗУЛЬТАТ:
    Линейная зависимость времени от количества матриц, так как каждая матрица
    обрабатывается независимо и за постоянное время.
    """
    print(f"\n--- Исследование времени выполнения для p={p} ---")

    # Формируем массив количества матриц для тестирования
    # Используем логарифмическую шкалу для равномерного покрытия диапазона
    ns = [10, 100, 1000, 5000, 10000]
    ns = [n for n in ns if n <= max_n]  # Убедимся, что не превышаем max_n

    print(f"Тестируемые количества матриц: {ns}")

    times = []  # Список для хранения времени выполнения

    for n in ns:
        print(f"Обработка {n} матриц...")

        # Генерируем n случайных матриц 2×2 с элементами из [0, 1)
        # uniform distribution обеспечивает разнообразие тестовых случаев
        matrices = [np.random.rand(2, 2) for _ in range(n)]

        # ЗАСЕКАЕМ ВРЕМЯ НАЧАЛА ВЫЧИСЛЕНИЙ
        start_time = time.time()

        # ВЫЧИСЛЯЕМ КОРЕНЬ СТЕПЕНИ P ДЛЯ КАЖДОЙ МАТРИЦЫ
        successful_calculations = 0
        for i, M in enumerate(matrices):
            try:
                matrix_pth_root(M, p)
                successful_calculations += 1
            except ValueError as e:
                # Пропускаем матрицы, для которых вычисление невозможно
                # (например, отрицательные собственные значения при чётном p)
                if i == 0:  # Выводим сообщение только для первой ошибки
                    print(f"  Предупреждение: пропущена матрица из-за {e}")
                continue

        # ВРЕМЯ ВЫПОЛНЕНИЯ ДЛЯ ТЕКУЩЕГО n
        execution_time = time.time() - start_time
        times.append(execution_time)

        print(f"  Успешно обработано: {successful_calculations}/{n} матриц")
        print(f"  Время выполнения: {execution_time:.4f} сек")

    # ПОСТРОЕНИЕ ГРАФИКА
    plt.figure(figsize=(10, 6))
    plt.plot(ns, times, 'bo-', linewidth=2, markersize=8, label='Экспериментальные данные')

    # Добавляем теоретическую линейную зависимость для сравнения
    if len(ns) > 1:
        # Аппроксимируем линейной функцией: time = k * n
        k = times[1] / ns[1]  # Коэффициент из второго измерения
        theoretical_times = [k * n for n in ns]
        plt.plot(ns, theoretical_times, 'r--', label='Теоретическая линейная зависимость')

    plt.xlabel('Количество матриц', fontsize=12)
    plt.ylabel('Время выполнения (сек)', fontsize=12)
    plt.title(f'Зависимость времени вычисления от количества матриц (p={p})', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()

    print("График построен!")

# ----------------------------------------------------------------------------
# Пункт 3: График относительной ошибки в зависимости от числа обусловленности
# ----------------------------------------------------------------------------

def error_experiment(p=3, num_matrices=1000):
    """
    ИССЛЕДОВАНИЕ ТОЧНОСТИ АЛГОРИТМА

    ЦЕЛЬ: Изучить, как ошибка вычисления зависит от числа обусловленности матрицы.
    Число обусловленности характеризует чувствительность решения к малым возмущениям.

    МЕТОДИКА:
    1. Генерируем "хорошие" матрицы M с диагональным преобладанием
    2. Вычисляем A = M^p (известная степень)
    3. Вычисляем корень A^(1/p) нашим алгоритмом
    4. Сравниваем результат с исходной матрицей M
    5. Анализируем зависимость ошибки от числа обусловленности A

    ТЕОРЕТИЧЕСКОЕ ОБОСНОВАНИЕ:
    Число обусловленности cond(A) = ||A||·||A^(-1)|| показывает, насколько
    ошибка во входных данных усиливается в ошибке результата.
    Большое число обусловленности означает плохую обусловленность задачи.
    """
    print(f"\n--- Исследование точности для p={p} ---")
    print(f"Генерация {num_matrices} тестовых матриц...")

    errors = []             # Относительные ошибки
    condition_numbers = []  # Числа обусловленности
    matrices_generated = 0  # Счётчик успешно сгенерированных матриц

    while matrices_generated < num_matrices:
        # ГЕНЕРАЦИЯ СЛУЧАЙНОЙ МАТРИЦЫ M
        # Используем целые числа для точности сравнения
        M = np.random.randint(-100, 100, (2, 2)).astype(float)

        # ОБЕСПЕЧИВАЕМ ДИАГОНАЛЬНОЕ ПРЕОБЛАДАНИЕ
        # Это гарантирует невырожденность и хорошие численные свойства
        M[0, 0] = max(abs(M[0, 0]), abs(M[0, 1])) + np.random.randint(1, 20)
        M[1, 1] = max(abs(M[1, 0]), abs(M[1, 1])) + np.random.randint(1, 20)

        # ВОЗВОДИМ M В СТЕПЕНЬ P: A = M^p
        try:
            A = np.linalg.matrix_power(M, p)
        except:
            continue  # Пропускаем проблемные матрицы

        # ВЫЧИСЛЯЕМ ЧИСЛО ОБУСЛОВЛЕННОСТИ
        cond_A = np.linalg.cond(A)

        # Пропускаем матрицы с экстремальными числами обусловленности
        if cond_A > 1e15 or cond_A < 1e-10:
            continue

        # ВЫЧИСЛЯЕМ КОРЕНЬ СТЕПЕНИ P ИЗ A
        try:
            A_root = matrix_pth_root(A, p)
        except ValueError:
            continue  # Пропускаем матрицы с проблемными собственными значениями

        # ВЫЧИСЛЯЕМ ОТНОСИТЕЛЬНУЮ ОШИБКУ
        # Истинный корень - исходная матрица M (поскольку A = M^p)
        true_root = M

        # Спектральная норма разности ||A_root - M||
        error_norm = norm(A_root - true_root, 2)
        # Спектральная норм истинного значения ||M||
        true_norm = norm(true_root, 2)

        # ОТНОСИТЕЛЬНАЯ ОШИБКА
        relative_error = error_norm / true_norm if true_norm > 0 else error_norm

        errors.append(relative_error)
        condition_numbers.append(cond_A)
        matrices_generated += 1

        if matrices_generated % 100 == 0:
            print(f"  Обработано {matrices_generated}/{num_matrices} матриц")

    print("Анализ зависимости ошибки от числа обусловленности...")

    # ПОСТРОЕНИЕ ГРАФИКА РАССЕЯНИЯ
    plt.figure(figsize=(10, 6))
    plt.scatter(condition_numbers, errors, alpha=0.5, s=20, c='red')

    # Добавляем линию тренда для визуализации общей зависимости
    if len(condition_numbers) > 1:
        # Логарифмируем данные для линейной регрессии
        log_cond = np.log10(condition_numbers)
        log_errors = np.log10(errors)

        # Линейная регрессия в логарифмических координатах
        coeffs = np.polyfit(log_cond, log_errors, 1)
        trend_line = 10**(coeffs[1] + coeffs[0] * log_cond)

        plt.plot(condition_numbers, trend_line, 'b-', linewidth=2,
                label=f'Тренд: ошибка ~ cond^{coeffs[0]:.2f}')

    plt.xlabel('Число обусловленности cond(A)', fontsize=12)
    plt.ylabel('Относительная ошибка', fontsize=12)
    plt.title(f'Зависимость ошибки от числа обусловленности (p={p})', fontsize=14)
    plt.xscale('log')  # Логарифмическая шкала для числа обусловленности
    plt.yscale('log')  # Логарифмическая шкала для ошибки
    plt.grid(True, alpha=0.3, which='both')
    plt.legend()
    plt.tight_layout()
    plt.show()

    # СТАТИСТИЧЕСКИЙ АНАЛИЗ
    print("\nСтатистика ошибок:")
    print(f"  Средняя ошибка: {np.mean(errors):.2e}")
    print(f"  Медианная ошибка: {np.median(errors):.2e}")
    print(f"  Максимальная ошибка: {np.max(errors):.2e}")
    print(f"  Минимальная ошибка: {np.min(errors):.2e}")

    print("График построен!")

# ----------------------------------------------------------------------------
# Пункт 4: Влияние возмущений на точность вычисления корня
# ----------------------------------------------------------------------------

def perturbation_experiment(A, p=3, num_perturbations=1000):
    """
    ИССЛЕДОВАНИЕ УСТОЙЧИВОСТИ АЛГОРИТМА К ВОЗМУЩЕНИЯМ

    ЦЕЛЬ: Изучить, как малые возмущения во входной матрице влияют на точность
    вычисления корня. Это важно для оценки численной устойчивости алгоритма.

    МЕТОДИКА:
    1. Берем исходную матрицу A и вычисляем "истинный" корень
    2. Генерируем множество малых случайных возмущений
    3. Для каждого возмущения вычисляем корень из A + возмущение
    4. Сравниваем с "истинным" корнем
    5. Анализируем зависимость ошибки от нормы возмущения

    ТЕОРЕТИЧЕСКОЕ ОБОСНОВАНИЕ:
    Устойчивый алгоритм должен давать малую ошибку при малых возмущениях.
    Наклон зависимости error vs perturbation показывает чувствительность алгоритма.
    """
    print(f"\n--- Исследование устойчивости к возмущениям ---")
    print(f"Исходная матрица A:")
    print(A)
    print(f"Число обусловленности: {np.linalg.cond(A):.2e}")

    # ВЫЧИСЛЯЕМ "ИСТИННЫЙ" КОРЕНЬ ИЗ ИСХОДНОЙ МАТРИЦЫ
    try:
        true_root = matrix_pth_root(A, p)
        print("Истинный корень вычислен успешно")
    except ValueError as e:
        print(f"Ошибка: невозможно вычислить корень из исходной матрицы: {e}")
        return

    # ГЕНЕРАЦИЯ СЛУЧАЙНЫХ ВОЗМУЩЕНИЙ
    # Возмущения распределены равномерно в диапазоне [-0.01, 0.01]
    perturbations = [np.random.uniform(-0.01, 0.01, (2, 2)) for _ in range(num_perturbations)]

    print(f"Сгенерировано {num_perturbations} возмущений")

    errors = []              # Нормы ошибок
    perturbation_norms = []  # Нормы возмущений

    # АНАЛИЗ ДЛЯ КАЖДОГО ВОЗМУЩЕНИЯ
    successful_calculations = 0
    for i, pert in enumerate(perturbations):
        # ФОРМИРУЕМ ВОЗМУЩЁННУЮ МАТРИЦУ
        A_pert = A + pert

        # ВЫЧИСЛЯЕМ КОРЕНЬ ИЗ ВОЗМУЩЁННОЙ МАТРИЦЫ
        try:
            root_pert = matrix_pth_root(A_pert, p)
            successful_calculations += 1
        except ValueError:
            continue  # Пропускаем проблемные возмущения

        # ВЫЧИСЛЯЕМ ОШИБКУ
        error = norm(root_pert - true_root, 2)
        errors.append(error)

        # ВЫЧИСЛЯЕМ НОРМУ ВОЗМУЩЕНИЯ
        pert_norm = norm(pert, 2)
        perturbation_norms.append(pert_norm)

        if i % 100 == 0 and i > 0:
            print(f"  Обработано {i}/{num_perturbations} возмущений")

    print(f"Успешно обработано: {successful_calculations}/{num_perturbations} возмущений")

    # ПОСТРОЕНИЕ ГРАФИКА
    plt.figure(figsize=(10, 6))
    plt.scatter(perturbation_norms, errors, alpha=0.5, s=20, c='green')

    # Добавляем линию тренда
    if len(perturbation_norms) > 1:
        coeffs = np.polyfit(perturbation_norms, errors, 1)
        trend_line = np.polyval(coeffs, perturbation_norms)
        plt.plot(perturbation_norms, trend_line, 'r-', linewidth=2,
                label=f'Тренд: наклон = {coeffs[0]:.2f}')

    plt.xlabel('Норма возмущения ||ΔA||', fontsize=12)
    plt.ylabel('Норма ошибки ||f(A+ΔA) - f(A)||', fontsize=12)
    plt.title(f'Чувствительность к возмущениям (cond(A)={np.linalg.cond(A):.2e})', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # АНАЛИЗ ЧУВСТВИТЕЛЬНОСТИ
    if len(errors) > 0:
        sensitivity = np.median([e/p for e, p in zip(errors, perturbation_norms) if p > 0])
        print(f"Медианная чувствительность: {sensitivity:.2f}")
        print("(Коэффициент усиления ошибки: во сколько раз ошибка результата")
        print(" больше ошибки во входных данных)")

    print("График построен!")

# ----------------------------------------------------------------------------
# Пункт 5: Зависимость времени вычисления и ошибки от степени p
# ----------------------------------------------------------------------------

def time_vs_accuracy(A, max_p=10):
    """
    ИССЛЕДОВАНИЕ ЗАВИСИМОСТИ ТОЧНОСТИ И ВРЕМЕНИ ОТ СТЕПЕНИ КОРНЯ

    ЦЕЛЬ: Изучить, как точность и время вычисления зависят от степени корня p.
    Это важно для понимания поведения алгоритма при различных параметрах.

    МЕТОДИКА:
    1. Для каждой степени p от 1 до max_p:
       - Вычисляем корень нашей функцией
       - Вычисляем эталонное значение с помощью scipy
       - Сравниваем результаты и измеряем время
    2. Строим графики зависимости времени и ошибки от p

    ТЕОРЕТИЧЕСКОЕ ОБОСНОВАНИЕ:
    - Время вычисления должно слабо зависеть от p
    - Ошибка может возрастать для больших p из-за накопления погрешностей
    """
    print(f"\n--- Зависимость точности и времени от степени корня ---")
    print(f"Тестовая матрица A:")
    print(A)

    times = []      # Время выполнения
    errors = []     # Относительные ошибки
    p_values = []   # Значения степени p

    # ТЕСТИРУЕМ РАЗЛИЧНЫЕ ЗНАЧЕНИЯ p
    for p in range(1, max_p + 1):
        print(f"Тестирование p={p}...")

        # ИЗМЕРЯЕМ ВРЕМЯ ВЫЧИСЛЕНИЯ НАШЕЙ ФУНКЦИЕЙ
        start_time = time.time()
        try:
            our_root = matrix_pth_root(A, p)
            our_time = time.time() - start_time
        except ValueError as e:
            print(f"  Пропуск p={p}: {e}")
            continue

        # ВЫЧИСЛЯЕМ ЭТАЛОННОЕ ЗНАЧЕНИЕ С ПОМОЩЬЮ SCIPY
        try:
            scipy_root = fractional_matrix_power(A, 1/p)
        except:
            print(f"  Ошибка scipy для p={p}")
            continue

        # ВЫЧИСЛЯЕМ ОТНОСИТЕЛЬНУЮ ОШИБКУ
        error = norm(our_root - scipy_root, 2) / norm(scipy_root, 2)

        times.append(our_time)
        errors.append(error)
        p_values.append(p)

        print(f"  Время: {our_time:.4f} сек, Ошибка: {error:.2e}")

    # ПОСТРОЕНИЕ ГРАФИКОВ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # График 1: Время выполнения
    ax1.plot(p_values, times, 'bo-', linewidth=2, markersize=6)
    ax1.set_xlabel('Степень корня p', fontsize=12)
    ax1.set_ylabel('Время выполнения (сек)', fontsize=12)
    ax1.set_title('Зависимость времени от степени корня', fontsize=14)
    ax1.grid(True, alpha=0.3)

    # График 2: Относительная ошибка
    ax2.plot(p_values, errors, 'ro-', linewidth=2, markersize=6)
    ax2.set_xlabel('Степень корня p', fontsize=12)
    ax2.set_ylabel('Относительная ошибка', fontsize=12)
    ax2.set_title('Зависимость ошибки от степени корня', fontsize=14)
    ax2.set_yscale('log')  # Логарифмическая шкала для лучшей визуализации
    ax2.grid(True, alpha=0.3, which='both')

    plt.tight_layout()
    plt.show()

    # СТАТИСТИЧЕСКИЙ АНАЛИЗ
    if errors:
        print(f"\nСтатистика по ошибкам для p=1..{max_p}:")
        print(f"  Средняя ошибка: {np.mean(errors):.2e}")
        print(f"  Максимальная ошибка: {np.max(errors):.2e} (при p={p_values[np.argmax(errors)]})")
        print(f"  Минимальная ошибка: {np.min(errors):.2e} (при p={p_values[np.argmin(errors)]})")

    print("Графики построены!")

# ============================================================================
# ЗАДАЧА 2: РЕГРЕССИЯ И РЕГУЛЯРИЗАЦИЯ
# ============================================================================

print("\n" + "=" * 80)
print("ЗАДАЧА 2: РЕГРЕССИЯ И РЕГУЛЯРИЗАЦИЯ")
print("=" * 80)

# ДАННЫЕ ПО ВЕЛИКОБРИТАНИИ (20 лет)
# Столбцы: y (потребление цыплят), x1 (среднедушевой доход),
#          x2 (цена цыплят), x3 (цена свинины), x4 (цена говядины)
data_nonlinear = np.array([
    [30.8, 459.7, 39.5, 55.3, 79.2],    # Год 1
    [31.2, 492.9, 37.3, 54.7, 77.4],    # Год 2
    [33.3, 528.6, 38.1, 63.7, 80.2],    # Год 3
    [35.6, 560.3, 39.3, 69.8, 80.4],    # Год 4
    [36.4, 624.6, 37.8, 65.9, 83.9],    # Год 5
    [36.7, 666.4, 38.4, 64.5, 85.5],    # Год 6
    [38.4, 717.8, 40.1, 70.0, 93.7],    # Год 7
    [40.4, 768.2, 38.6, 73.2, 106.1],   # Год 8
    [40.3, 843.3, 39.8, 67.8, 104.8],   # Год 9
    [41.8, 911.6, 39.7, 79.1, 114.0],   # Год 10
    [40.4, 931.1, 52.1, 95.4, 124.1],   # Год 11
    [40.7, 1021.5, 48.9, 94.2, 127.6],  # Год 12
    [40.1, 1165.9, 58.3, 123.5, 142.9], # Год 13
    [42.7, 1349.6, 57.9, 129.9, 143.6], # Год 14
    [44.1, 1449.4, 56.5, 117.6, 139.2], # Год 15
    [46.7, 1575.5, 63.7, 130.9, 165.5], # Год 16
    [50.6, 1759.1, 61.6, 129.8, 203.3], # Год 17
    [50.1, 1994.2, 58.9, 128.0, 219.6], # Год 18
    [51.7, 2258.1, 66.4, 141.0, 221.6], # Год 19
    [52.9, 2478.7, 70.4, 168.2, 232.6]  # Год 20
])

print("Исходные данные (20 наблюдений):")
print("Столбцы: y, x1, x2, x3, x4")
print(data_nonlinear)

# ПОДГОТОВКА ДАННЫХ ДЛЯ НЕЛИНЕЙНОЙ РЕГРЕССИИ
# Модель: y = β₀ · x₂^β₂ · x₃^β₁ · x₄^β₄
# После логарифмирования: log(y) = log(β₀) + β₂·log(x₂) + β₁·log(x₃) + β₄·log(x₄)

X = data_nonlinear[:, 2:]  # Признаки: x2, x3, x4 (столбцы 2, 3, 4)
y = data_nonlinear[:, 0]   # Целевая переменная: y (столбец 0)

print(f"\nПризнаки X (x2, x3, x4): {X.shape}")
print(f"Целевая переменная y: {y.shape}")

# ЛОГАРИФМИРУЕМ ДАННЫЕ ДЛЯ ЛИНЕАРИЗАЦИИ МОДЕЛИ
X_log = np.log(X)  # log(x2), log(x3), log(x4)
y_log = np.log(y)  # log(y)

print("\nЛогарифмированные данные:")
print("X_log (log(x2), log(x3), log(x4)):")
print(X_log)
print("y_log (log(y)):")
print(y_log)

# ФОРМИРУЕМ ИНДЕКСЫ ДЛЯ k-fold КРОСС-ВАЛИДАЦИИ ПО ЗАДАННОЙ СХЕМЕ
# Согласно заданию: (1,5,9,13,17), (2,6,10,14,18), (3,7,11,15), (4,8,12,16,20)
# В Python индексы начинаются с 0, поэтому вычитаем 1
fold_indices = [
    [0, 4, 8, 12, 16],    # (1,5,9,13,17)
    [1, 5, 9, 13, 17],    # (2,6,10,14,18)
    [2, 6, 10, 14],       # (3,7,11,15)
    [3, 7, 11, 15, 19]    # (4,8,12,16,20)
]

print(f"\nРазбиение на фолды для кросс-валидации:")
for i, fold in enumerate(fold_indices, 1):
    print(f"  Фолд {i}: наблюдения {[x+1 for x in fold]}")

# ВСПОМОГАТЕЛЬНАЯ ФУНКЦИЯ ДЛЯ КРОСС-ВАЛИДАЦИИ
def cv_mse(model_class, **model_kwargs):
    """
    ВЫПОЛНЕНИЕ k-fold КРОСС-ВАЛИДАЦИИ С ВЫЧИСЛЕНИЕМ MSE

    ЦЕЛЬ: Оценить качество модели на независимых тестовых выборках.

    МЕТОДИКА:
    - Разбиваем данные на k фолдов
    - По очереди каждый фолд используется как тестовая выборка
    - Остальные фолды - обучающая выборка
    - Вычисляем MSE на тестовой выборке
    - Усредняем MSE по всем фолдам

    Параметры:
    ----------
    model_class : класс sklearn
        Класс модели (LinearRegression, Ridge, Lasso)
    model_kwargs : dict
        Параметры для инициализации модели

    Возвращает:
    -----------
    float
        Средняя MSE по всем фолдам
    """
    mse_scores = []  # MSE для каждого фолда

    # ИТЕРИРУЕМСЯ ПО 4 ФОЛДАМ
    for k in range(4):
        # k-Й ФОЛД ИСПОЛЬЗУЕТСЯ КАК ТЕСТОВАЯ ВЫБОРКА
        test_idx = fold_indices[k]

        # ОСТАЛЬНЫЕ ФОЛДЫ ОБЪЕДИНЯЮТСЯ В ОБУЧАЮЩУЮ ВЫБОРКУ
        train_idx = np.setdiff1d(np.arange(20), test_idx)

        # РАЗДЕЛЯЕМ ДАННЫЕ НА ОБУЧАЮЩУЮ И ТЕСТОВУЮ ВЫБОРКИ
        X_train, X_test = X_log[train_idx], X_log[test_idx]
        y_train, y_test = y_log[train_idx], y_log[test_idx]

        # СТАНДАРТИЗАЦИЯ ПРИЗНАКОВ
        # Важно для регуляризованных моделей, чтобы штраф применялся равномерно
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)   # Обучаем scaler на train
        X_test_scaled = scaler.transform(X_test)         # Применяем к test

        # ДОБАВЛЯЕМ СТОЛБЕЦ ЕДИНИЦ ДЛЯ СВОБОДНОГО ЧЛЕНА
        # В линейной модели: y = β₀ + β₁·x₁ + β₂·x₂ + β₃·x₃
        # Столбец единиц соответствует коэффициенту β₀
        X_train_final = np.column_stack([np.ones(len(X_train_scaled)), X_train_scaled])
        X_test_final = np.column_stack([np.ones(len(X_test_scaled)), X_test_scaled])

        # СОЗДАЁМ И ОБУЧАЕМ МОДЕЛЬ
        model = model_class(**model_kwargs)
        model.fit(X_train_final, y_train)

        # ПРЕДСКАЗЫВАЕМ НА ТЕСТОВОЙ ВЫБОРКЕ
        y_pred = model.predict(X_test_final)

        # ВЫЧИСЛЯЕМ MSE (СРЕДНЕКВАДРАТИЧНУЮ ОШИБКУ)
        mse = mean_squared_error(y_test, y_pred)
        mse_scores.append(mse)

    # ВОЗВРАЩАЕМ СРЕДНЮЮ MSE ПО ВСЕМ ФОЛДАМ
    return np.mean(mse_scores)

# ----------------------------------------------------------------------------
# Пункт 1: Регрессия без регуляризации
# ----------------------------------------------------------------------------

print("\n" + "-" * 70)
print("1. РЕГРЕССИЯ БЕЗ РЕГУЛЯРИЗАЦИИ")
print("-" * 70)

# Вычисляем MSE для обычной линейной регрессии (без регуляризации)
mse_no_reg = cv_mse(LinearRegression)
print(f"MSE без регуляризации: {mse_no_reg:.6f}")

# ОБУЧАЕМ МОДЕЛЬ НА ВСЕХ ДАННЫХ ДЛЯ АНАЛИЗА КОЭФФИЦИЕНТОВ
scaler_full = StandardScaler()
X_log_scaled = scaler_full.fit_transform(X_log)
X_log_final = np.column_stack([np.ones(len(X_log_scaled)), X_log_scaled])

model_no_reg = LinearRegression()
model_no_reg.fit(X_log_final, y_log)

print("Коэффициенты модели без регуляризации:")
print(f"  Свободный член (β₀): {model_no_reg.intercept_:.4f}")
print(f"  Коэффициенты: {model_no_reg.coef_[1:]}")  # Игнорируем первый коэффициент (соответствует столбцу единиц)

# ----------------------------------------------------------------------------
# Пункт 2: L2-регуляризация (Ridge)
# ----------------------------------------------------------------------------

print("\n" + "-" * 70)
print("2. L2-РЕГУЛЯРИЗАЦИЯ (RIDGE)")
print("-" * 70)

# ДИАПАЗОН ЗНАЧЕНИЙ ПАРАМЕТРА РЕГУЛЯРИЗАЦИИ ALPHA
# L2-регуляризация добавляет штраф за большие значения коэффициентов
# α управляет силой регуляризации: большие α → больший штраф → меньшие коэффициенты
alphas = np.logspace(-3, 3, 50)  # От 0.001 до 1000 в логарифмической шкале

print(f"Тестирование {len(alphas)} значений alpha...")

# ВЫЧИСЛЯЕМ MSE ДЛЯ КАЖДОГО ЗНАЧЕНИЯ ALPHA
ridge_scores = []
for i, alpha in enumerate(alphas):
    mse = cv_mse(Ridge, alpha=alpha)
    ridge_scores.append(mse)
    if i % 10 == 0:
        print(f"  alpha={alpha:.4f}, MSE={mse:.6f}")

# НАХОДИМ ОПТИМАЛЬНОЕ ЗНАЧЕНИЕ ALPHA (МИНИМАЛЬНАЯ MSE)
best_ridge_alpha = alphas[np.argmin(ridge_scores)]
best_ridge_mse = np.min(ridge_scores)

print(f"\nЛучшее значение alpha: {best_ridge_alpha:.4f}")
print(f"MSE при лучшем alpha: {best_ridge_mse:.6f}")

# ОБУЧАЕМ МОДЕЛЬ С ОПТИМАЛЬНЫМ ALPHA ДЛЯ АНАЛИЗА КОЭФФИЦИЕНТОВ
model_ridge = Ridge(alpha=best_ridge_alpha)
model_ridge.fit(X_log_final, y_log)

print("Коэффициенты Ridge-модели:")
print(f"  Свободный член (β₀): {model_ridge.intercept_:.4f}")
print(f"  Коэффициенты: {model_ridge.coef_[1:]}")

# СТРОИМ ГРАФИК ЗАВИСИМОСТИ MSE ОТ ALPHA
plt.figure(figsize=(10, 6))
plt.plot(alphas, ridge_scores, linewidth=2, label='Ridge (L2)')
plt.axvline(best_ridge_alpha, color='red', linestyle='--',
            label=f'Оптимальное alpha={best_ridge_alpha:.4f}')
plt.xscale('log')  # Логарифмическая шкала по оси X
plt.xlabel('Alpha (параметр регуляризации)', fontsize=12)
plt.ylabel('MSE (кросс-валидация)', fontsize=12)
plt.title('L2-регуляризация: зависимость качества от alpha', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ----------------------------------------------------------------------------
# Пункт 3: L1-регуляризация (Lasso)
# ----------------------------------------------------------------------------

print("\n" + "-" * 70)
print("3. L1-РЕГУЛЯРИЗАЦИЯ (LASSO)")
print("-" * 70)

# L1-регуляризация добавляет штраф за ненулевые коэффициенты
# Это приводит к разреженным решениям (некоторые коэффициенты становятся точно 0)

print(f"Тестирование {len(alphas)} значений alpha...")

# ВЫЧИСЛЯЕМ MSE ДЛЯ КАЖДОГО ЗНАЧЕНИЯ ALPHA
# max_iter увеличен для гарантии сходимости
lasso_scores = []
for i, alpha in enumerate(alphas):
    mse = cv_mse(Lasso, alpha=alpha, max_iter=10000)
    lasso_scores.append(mse)
    if i % 10 == 0:
        print(f"  alpha={alpha:.4f}, MSE={mse:.6f}")

# НАХОДИМ ОПТИМАЛЬНОЕ ЗНАЧЕНИЕ ALPHA
best_lasso_alpha = alphas[np.argmin(lasso_scores)]
best_lasso_mse = np.min(lasso_scores)

print(f"\nЛучшее значение alpha: {best_lasso_alpha:.4f}")
print(f"MSE при лучшем alpha: {best_lasso_mse:.6f}")

# ОБУЧАЕМ МОДЕЛЬ С ОПТИМАЛЬНЫМ ALPHA ДЛЯ АНАЛИЗА КОЭФФИЦИЕНТОВ
model_lasso = Lasso(alpha=best_lasso_alpha, max_iter=10000)
model_lasso.fit(X_log_final, y_log)

print("Коэффициенты Lasso-модели:")
print(f"  Свободный член (β₀): {model_lasso.intercept_:.4f}")
print(f"  Коэффициенты: {model_lasso.coef_[1:]}")

# Проверяем, какие признаки были отсечены (коэффициент = 0)
nonzero_coeffs = np.sum(model_lasso.coef_[1:] != 0)
print(f"Количество ненулевых коэффициентов: {nonzero_coeffs} из {len(model_lasso.coef_[1:])}")

# СТРОИМ ГРАФИК ЗАВИСИМОСТИ MSE ОТ ALPHA
plt.figure(figsize=(10, 6))
plt.plot(alphas, lasso_scores, linewidth=2, color='orange', label='Lasso (L1)')
plt.axvline(best_lasso_alpha, color='red', linestyle='--',
            label=f'Оптимальное alpha={best_lasso_alpha:.4f}')
plt.xscale('log')
plt.xlabel('Alpha (параметр регуляризации)', fontsize=12)
plt.ylabel('MSE (кросс-валидация)', fontsize=12)
plt.title('L1-регуляризация: зависимость качества от alpha', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ----------------------------------------------------------------------------
# СРАВНЕНИЕ РЕЗУЛЬТАТОВ
# ----------------------------------------------------------------------------

print("\n" + "=" * 70)
print("СРАВНЕНИЕ РЕЗУЛЬТАТОВ")
print("=" * 70)

print("МЕТОД               |    MSE    |  ЛУЧШИЙ ALPHA")
print("-" * 50)
print(f"Без регуляризации  | {mse_no_reg:.6f} |     -")
print(f"Ridge (L2)         | {best_ridge_mse:.6f} | {best_ridge_alpha:.4f}")
print(f"Lasso (L1)         | {best_lasso_mse:.6f} | {best_lasso_alpha:.4f}")

# АНАЛИЗ КОЭФФИЦИЕНТОВ
print("\nСРАВНЕНИЕ КОЭФФИЦИЕНТОВ:")
print("Признак | Без регул. |   Ridge   |   Lasso   ")
print("-" * 45)
features = ['log(x₂)', 'log(x₃)', 'log(x₄)']
for i, feature in enumerate(features):
    print(f"{feature:7} | {model_no_reg.coef_[1+i]:10.4f} | {model_ridge.coef_[1+i]:10.4f} | {model_lasso.coef_[1+i]:10.4f}")

# ВЫВОДЫ
print("\nВЫВОДЫ:")
if best_ridge_mse < mse_no_reg and best_lasso_mse < mse_no_reg:
    print("• Регуляризация улучшила качество модели")
    if best_ridge_mse < best_lasso_mse:
        print("• L2-регуляризация показала лучший результат")
    else:
        print("• L1-регуляризация показала лучший результат")
else:
    print("• Регуляризация не улучшила качество модели")

# Проверяем, отсек ли Lasso какие-то признаки
if np.any(model_lasso.coef_[1:] == 0):
    zero_features = [features[i] for i in range(len(features)) if model_lasso.coef_[1+i] == 0]
    print(f"• Lasso отсек признаки: {', '.join(zero_features)}")

# ============================================================================
# ЗАДАЧА 3: ВАВИЛОНСКИЙ МЕТОД И МЕТОД НЬЮТОНА
# ============================================================================

print("\n" + "=" * 80)
print("ЗАДАЧА 3: ВАВИЛОНСКИЙ МЕТОД И МЕТОД НЬЮТОНА")
print("=" * 80)

def babylonian_sqrt(S, initial_guess, tolerance=1e-10, max_iter=1000):
    """
    ВАВИЛОНСКИЙ МЕТОД ВЫЧИСЛЕНИЯ КВАДРАТНОГО КОРНЯ

    ИСТОРИЧЕСКАЯ СПРАВКА:
    Один из древнейших алгоритмов, известный ещё вавилонянам около 2000 лет до н.э.
    Также известен как Геронов метод.

    МАТЕМАТИЧЕСКАЯ ОСНОВА:
    Итерационная формула: x_{n+1} = (1/2) * (x_n + S/x_n)

    ГЕОМЕТРИЧЕСКАЯ ИНТЕРПРЕТАЦИЯ:
    Если x_n - приближение к √S, то S/x_n - другое приближение.
    Их среднее арифметическое даёт лучшее приближение.

    СВОЙСТВА:
    - Квадратичная сходимость (удваивает число верных знаков на итерацию)
    - Сходится при любом положительном начальном приближении
    - Не требует вычисления производных

    НЕДОСТАТКИ:
    - Использует операцию деления (медленнее умножения)

    Параметры:
    ----------
    S : float
        Число, из которого извлекается корень (S > 0)
    initial_guess : float
        Начальное приближение (x₀ > 0)
    tolerance : float
        Точность (условие остановки: |x_{n+1} - x_n| < tolerance)
    max_iter : int
        Максимальное число итераций (защита от зацикливания)

    Возвращает:
    -----------
    float
        Приближённое значение √S
    """
    # ПРОВЕРКА КОРРЕКТНОСТИ ВХОДНЫХ ДАННЫХ
    if S <= 0:
        raise ValueError("S должно быть положительным числом")
    if initial_guess <= 0:
        raise ValueError("Начальное приближение должно быть положительным")

    x = initial_guess  # Текущее приближение
    iterations = 0     # Счётчик итераций

    print(f"Вавилонский метод для √{S} с начальным приближением {initial_guess}")

    # ОСНОВНОЙ ЦИКЛ ИТЕРАЦИЙ
    for iteration in range(max_iter):
        # ВЫЧИСЛЯЕМ СЛЕДУЮЩЕЕ ПРИБЛИЖЕНИЕ ПО ФОРМУЛЕ ВАВИЛОНА
        x_new = 0.5 * (x + S / x)
        iterations += 1

        # ВЫВОДИМ ИНФОРМАЦИЮ О ТЕКУЩЕЙ ИТЕРАЦИИ (только первые несколько)
        if iteration < 5:
            error = abs(x_new - np.sqrt(S))
            print(f"  Итерация {iteration}: x = {x_new:.10f}, ошибка = {error:.2e}")

        # ПРОВЕРЯЕМ УСЛОВИЕ СХОДИМОСТИ
        if abs(x_new - x) < tolerance:
            print(f"Сходимость достигнута за {iterations} итераций")
            print(f"Результат: {x_new:.10f}")
            print(f"Точное значение: {np.sqrt(S):.10f}")
            print(f"Финальная ошибка: {abs(x_new - np.sqrt(S)):.2e}")
            return x_new

        # ПЕРЕХОДИМ К СЛЕДУЮЩЕЙ ИТЕРАЦИИ
        x = x_new

    # ЕСЛИ ДОСТИГНУТО МАКСИМАЛЬНОЕ ЧИСЛО ИТЕРАЦИЙ
    print(f"Предупреждение: достигнуто максимальное число итераций ({max_iter})")
    return x

def newton_sqrt(S, initial_guess, tolerance=1e-10, max_iter=1000):
    """
    МЕТОД НЬЮТОНА ДЛЯ ВЫЧИСЛЕНИЯ КВАДРАТНОГО КОРНЯ

    МАТЕМАТИЧЕСКАЯ ОСНОВА:
    Решаем уравнение f(x) = x² - S = 0 методом Ньютона.

    Итерационная формула Ньютона:
    x_{n+1} = x_n - f(x_n)/f'(x_n)

    Для f(x) = x² - S:
    f'(x) = 2x
    x_{n+1} = x_n - (x_n² - S)/(2x_n) = (x_n + S/x_n)/2

    ИНТЕРЕСНЫЙ ФАКТ:
    Для квадратного корня метод Ньютона даёт ту же формулу,
    что и вавилонский метод! Но подходы разные:
    - Вавилонский: геометрическая интерпретация
    - Ньютона: аналитическая интерпретация через производные

    Преимущества:
    - Квадратичная скорость сходимости
    - Обобщается на другие функции

    Недостатки:
    - Требует вычисления производной
    - Может не сходиться при плохом начальном приближении

    Параметры:
    ----------
    S : float
        Число, из которого извлекается корень (S > 0)
    initial_guess : float
        Начальное приближение (должно быть близко к √S)
    tolerance : float
        Точность (условие остановки: |x_{n+1} - x_n| < tolerance)
    max_iter : int
        Максимальное число итераций

    Возвращает:
    -----------
    float
        Приближённое значение √S
    """
    if S <= 0:
        raise ValueError("S должно быть положительным числом")

    x = initial_guess
    iterations = 0

    print(f"Метод Ньютона для √{S} с начальным приближением {initial_guess}")

    for iteration in range(max_iter):
        # ВЫЧИСЛЯЕМ ЗНАЧЕНИЕ ФУНКЦИИ И ПРОИЗВОДНОЙ
        f = x**2 - S        # f(x) = x² - S
        f_derivative = 2 * x  # f'(x) = 2x

        # ИЗБЕГАЕМ ДЕЛЕНИЯ НА НОЛЬ
        if abs(f_derivative) < 1e-15:
            break

        # ДЕЛАЕМ ШАГ МЕТОДА НЬЮТОНА
        x_new = x - f / f_derivative
        iterations += 1

        # ВЫВОДИМ ИНФОРМАЦИЮ О ТЕКУЩЕЙ ИТЕРАЦИИ
        if iteration < 5:
            error = abs(x_new - np.sqrt(S))
            print(f"  Итерация {iteration}: x = {x_new:.10f}, ошибка = {error:.2e}")

        # ПРОВЕРЯЕМ УСЛОВИЕ СХОДИМОСТИ
        if abs(x_new - x) < tolerance:
            print(f"Сходимость достигнута за {iterations} итераций")
            print(f"Результат: {x_new:.10f}")
            print(f"Точное значение: {np.sqrt(S):.10f}")
            print(f"Финальная ошибка: {abs(x_new - np.sqrt(S)):.2e}")
            return x_new

        x = x_new

    print(f"Предупреждение: достигнуто максимальное число итераций ({max_iter})")
    return x

def combined_sqrt(S, tolerance=1e-10):
    """
    КОМБИНИРОВАННЫЙ МЕТОД ВЫЧИСЛЕНИЯ КВАДРАТНОГО КОРНЯ

    СТРАТЕГИЯ:
    1. Используем несколько итераций вавилонского метода для получения
       хорошего начального приближения (гарантированно сходится)
    2. Уточняем результат методом Ньютона (быстрая сходимость)

    ПРЕИМУЩЕСТВА:
    - Гарантированная сходимость (благодаря вавилонскому методу)
    - Быстрая скорость сходимости (благодаря методу Ньютона)
    - Универсальность (работает для любых начальных приближений)

    Параметры:
    ----------
    S : float
        Число, из которого извлекается корень (S > 0)
    tolerance : float
        Требуемая точность

    Возвращает:
    -----------
    float
        Приближённое значение √S
    """
    if S <= 0:
        raise ValueError("S должно быть положительным числом")

    print(f"\nКОМБИНИРОВАННЫЙ МЕТОД ДЛЯ √{S}")
    print("=" * 50)

    # ЭТАП 1: ГРУБОЕ НАЧАЛЬНОЕ ПРИБЛИЖЕНИЕ МЕТОДОМ ВАВИЛОНА
    # Выбираем начальное приближение в зависимости от величины S
    if S > 1:
        initial_guess = S / 2      # Для больших S
    elif S < 1 and S > 0:
        initial_guess = 1          # Для малых S
    else:
        initial_guess = 1          # По умолчанию

    print("ЭТАП 1: Вавилонский метод для начального приближения")
    # Используем меньшую точность для быстрого получения приближения
    rough_approximation = babylonian_sqrt(S, initial_guess, tolerance=1e-5, max_iter=10)

    print(f"Получено начальное приближение: {rough_approximation:.10f}")

    # ЭТАП 2: УТОЧНЕНИЕ МЕТОДОМ НЬЮТОНА
    print("\nЭТАП 2: Уточнение методом Ньютона")
    final_result = newton_sqrt(S, rough_approximation, tolerance=tolerance)

    print(f"\nФИНАЛЬНЫЙ РЕЗУЛЬТАТ: √{S} = {final_result:.10f}")
    print(f"ПРОВЕРКА: ({final_result:.10f})² = {final_result**2:.10f}")

    return final_result

# ТЕСТИРОВАНИЕ КОМБИНИРОВАННОГО МЕТОДА
print("\nТЕСТИРОВАНИЕ КОМБИНИРОВАННОГО МЕТОДА:")
print("=" * 70)

# Разнообразные тестовые случаи
test_values = [
    25,        # Простой случай
    2,         # Иррациональное число
    1000,      # Большое число
    0.01,      # Малое число
    123456789, # Очень большое число
    0.0001     # Очень малое число
]

print("ТЕСТОВЫЕ ЗНАЧЕНИЯ:")
for S in test_values:
    print(f"\n--- Вычисление √{S} ---")
    try:
        result = combined_sqrt(S)
        exact = np.sqrt(S)
        error = abs(result - exact)
        print(f"ОКОНЧАТЕЛЬНАЯ ОШИБКА: {error:.2e}")

        # Проверяем точность
        if error < 1e-8:
            print("✅ ВЫСОКАЯ ТОЧНОСТЬ")
        elif error < 1e-5:
            print("⚠️  УДОВЛЕТВОРИТЕЛЬНАЯ ТОЧНОСТЬ")
        else:
            print("❌ НИЗКАЯ ТОЧНОСТЬ")

    except ValueError as e:
        print(f"❌ ОШИБКА: {e}")

# ТЕСТИРОВАНИЕ С РАЗНЫМИ НАЧАЛЬНЫМИ ПРИБЛИЖЕНИЯМИ
print("\n" + "=" * 70)
print("ТЕСТИРОВАНИЕ УСТОЙЧИВОСТИ К НАЧАЛЬНОМУ ПРИБЛИЖЕНИЮ")
print("=" * 70)

S_test = 16  # √16 = 4
initial_guesses = [1, 100, 0.1, 0.001, 1000]

print(f"Вычисление √{S_test} с разными начальными приближениями:")
for x0 in initial_guesses:
    print(f"\nНачальное приближение: {x0}")
    try:
        result = combined_sqrt(S_test)
        error = abs(result - 4)
        print(f"Результат: {result}, ошибка: {error:.2e}")
    except Exception as e:
        print(f"Ошибка: {e}")

print("\n" + "=" * 80)
print("ВСЕ ЗАДАЧИ ВЫПОЛНЕНЫ")
print("=" * 80)

# ============================================================================
# ЗАПУСК ЭКСПЕРИМЕНТОВ (раскомментируйте нужные строки)
# ============================================================================

# # ЗАДАЧА 1
# print("\n--- Запуск экспериментов для Задачи 1 ---")

# # Тестирование функции корня матрицы
# print("\nТестирование функции matrix_pth_root:")
# A_test = np.array([[2, 1], [1, 2]])
# p_test = 3
# try:
#     result = matrix_pth_root(A_test, p_test)
#     print("Тест пройден успешно!")
# except Exception as e:
#     print(f"Ошибка: {e}")

# # Пункт 2: График времени работы
# time_experiment(p=3, max_n=10000)

# # Пункт 3: График ошибки
# error_experiment(p=3)

# # Пункт 4: Влияние возмущений
# # Матрица с большим числом обусловленности
# A_bad = np.array([[1000, 999], [999, 998]])
# print(f"Число обусловленности A_bad: {np.linalg.cond(A_bad):.2e}")
# perturbation_experiment(A_bad, p=3)

# # Матрица с малым числом обусловленности
# A_good = np.eye(2)
# print(f"Число обусловленности A_good: {np.linalg.cond(A_good):.2e}")
# perturbation_experiment(A_good, p=3)

# # Пункт 5: Зависимость времени от p
# A_test = np.array([[2, 1], [1, 2]])
# time_vs_accuracy(A_test, max_p=10)